Namespace(base model='vgg', batch_size=2, cuda=True, data_dir='/workspace2/cth/VOCdevkit/VOC2012/', dataset='VOC2012', epoch=100, img_size=224, log_interval=20, lr=0.0001, name='PASCAL_ALPHA', num_class=20, save_interval=5, val_interval=5)
----------- Finished Dataset Preloading -----------
----- Finish Creating Patches ------
----------- Finished Dataset Preloading -----------
----- Finish Creating Patches ------
DPL(
  (cnn): VGG16(
    (layers): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (2): ReLU(inplace)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (5): ReLU(inplace)
      (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (9): ReLU(inplace)
      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (12): ReLU(inplace)
      (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (16): ReLU(inplace)
      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (19): ReLU(inplace)
      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (26): ReLU(inplace)
      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (29): ReLU(inplace)
      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (32): ReLU(inplace)
      (33): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (36): ReLU(inplace)
      (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (39): ReLU(inplace)
      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (42): ReLU(inplace)
    )
  )
  (roi_align): ROIAlign(
    (align): RoIAlign(
    )
  )
  (patch_pooling): PatchPooling(
  )
  (fcs): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): LeakyReLU(0.02, inplace)
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=1024, bias=True)
    (4): LeakyReLU(0.02, inplace)
    (5): Dropout(p=0.5)
  )
  (out): Linear(in_features=1024, out_features=20, bias=True)
)
---------- DPL Model Init Finished -----------
starting to train
[0/100][19/2859] Loss: 7.886239
[0/100][39/2859] Loss: 7.973185
[0/100][59/2859] Loss: 7.707825
[0/100][79/2859] Loss: 7.381243
[0/100][99/2859] Loss: 7.095672
[0/100][119/2859] Loss: 6.992971
[0/100][139/2859] Loss: 6.911751
[0/100][159/2859] Loss: 6.731612
[0/100][179/2859] Loss: 6.628502
[0/100][199/2859] Loss: 6.575608
[0/100][219/2859] Loss: 6.540296
[0/100][239/2859] Loss: 6.469949
[0/100][259/2859] Loss: 6.354907
[0/100][279/2859] Loss: 6.278477
[0/100][299/2859] Loss: 6.242486
[0/100][319/2859] Loss: 6.187586
[0/100][339/2859] Loss: 6.118039
[0/100][359/2859] Loss: 6.068495
[0/100][379/2859] Loss: 6.034809
[0/100][399/2859] Loss: 6.042469
[0/100][419/2859] Loss: 6.042497
[0/100][439/2859] Loss: 6.037598
[0/100][459/2859] Loss: 6.025674
[0/100][479/2859] Loss: 6.006980
[0/100][499/2859] Loss: 5.975028
[0/100][519/2859] Loss: 5.958428
[0/100][539/2859] Loss: 5.943124
[0/100][559/2859] Loss: 5.921396
[0/100][579/2859] Loss: 5.894643
[0/100][599/2859] Loss: 5.874886
[0/100][619/2859] Loss: 5.831856
[0/100][639/2859] Loss: 5.797312
[0/100][659/2859] Loss: 5.790550
[0/100][679/2859] Loss: 5.772631
[0/100][699/2859] Loss: 5.747968
[0/100][719/2859] Loss: 5.732878
[0/100][739/2859] Loss: 5.712949
[0/100][759/2859] Loss: 5.694438
[0/100][779/2859] Loss: 5.692529
[0/100][799/2859] Loss: 5.677590
[0/100][819/2859] Loss: 5.660277
[0/100][839/2859] Loss: 5.653741
[0/100][859/2859] Loss: 5.650791
[0/100][879/2859] Loss: 5.640239
[0/100][899/2859] Loss: 5.625810
[0/100][919/2859] Loss: 5.626645
[0/100][939/2859] Loss: 5.614796
[0/100][959/2859] Loss: 5.608099
[0/100][979/2859] Loss: 5.597917
[0/100][999/2859] Loss: 5.579784
[0/100][1019/2859] Loss: 5.562723
[0/100][1039/2859] Loss: 5.559588
[0/100][1059/2859] Loss: 5.541302
[0/100][1079/2859] Loss: 5.533675
[0/100][1099/2859] Loss: 5.527000
[0/100][1119/2859] Loss: 5.519489
[0/100][1139/2859] Loss: 5.512368
[0/100][1159/2859] Loss: 5.505587
[0/100][1179/2859] Loss: 5.512265
[0/100][1199/2859] Loss: 5.499954
[0/100][1219/2859] Loss: 5.489056
[0/100][1239/2859] Loss: 5.484647
[0/100][1259/2859] Loss: 5.479289
[0/100][1279/2859] Loss: 5.470657
[0/100][1299/2859] Loss: 5.470756
[0/100][1319/2859] Loss: 5.462353
[0/100][1339/2859] Loss: 5.454489
[0/100][1359/2859] Loss: 5.453740
[0/100][1379/2859] Loss: 5.452623
[0/100][1399/2859] Loss: 5.441437
[0/100][1419/2859] Loss: 5.433434
[0/100][1439/2859] Loss: 5.434929
[0/100][1459/2859] Loss: 5.425464
[0/100][1479/2859] Loss: 5.421061
[0/100][1499/2859] Loss: 5.414469
[0/100][1519/2859] Loss: 5.406833
[0/100][1539/2859] Loss: 5.401180
[0/100][1559/2859] Loss: 5.396098
[0/100][1579/2859] Loss: 5.382285
[0/100][1599/2859] Loss: 5.371208
[0/100][1619/2859] Loss: 5.361159
[0/100][1639/2859] Loss: 5.363666
[0/100][1659/2859] Loss: 5.359164
[0/100][1679/2859] Loss: 5.353492
[0/100][1699/2859] Loss: 5.349195
[0/100][1719/2859] Loss: 5.345077
[0/100][1739/2859] Loss: 5.346961
[0/100][1759/2859] Loss: 5.339526
[0/100][1779/2859] Loss: 5.333490
[0/100][1799/2859] Loss: 5.330840
[0/100][1819/2859] Loss: 5.332615
[0/100][1839/2859] Loss: 5.335979
[0/100][1859/2859] Loss: 5.335183
[0/100][1879/2859] Loss: 5.330428
[0/100][1899/2859] Loss: 5.326897
[0/100][1919/2859] Loss: 5.328411
[0/100][1939/2859] Loss: 5.329538
[0/100][1959/2859] Loss: 5.324460
[0/100][1979/2859] Loss: 5.327125
[0/100][1999/2859] Loss: 5.325085
[0/100][2019/2859] Loss: 5.324843
[0/100][2039/2859] Loss: 5.323247
[0/100][2059/2859] Loss: 5.318895
[0/100][2079/2859] Loss: 5.313873
[0/100][2099/2859] Loss: 5.312854
[0/100][2119/2859] Loss: 5.312082
[0/100][2139/2859] Loss: 5.305641
[0/100][2159/2859] Loss: 5.301517
[0/100][2179/2859] Loss: 5.299073
[0/100][2199/2859] Loss: 5.305442
[0/100][2219/2859] Loss: 5.303746
[0/100][2239/2859] Loss: 5.302597
[0/100][2259/2859] Loss: 5.300807
[0/100][2279/2859] Loss: 5.299492
[0/100][2299/2859] Loss: 5.298727
[0/100][2319/2859] Loss: 5.297397
[0/100][2339/2859] Loss: 5.297468
[0/100][2359/2859] Loss: 5.293554
[0/100][2379/2859] Loss: 5.292545
[0/100][2399/2859] Loss: 5.288197
[0/100][2419/2859] Loss: 5.286437
[0/100][2439/2859] Loss: 5.282591
[0/100][2459/2859] Loss: 5.280897
[0/100][2479/2859] Loss: 5.276097
[0/100][2499/2859] Loss: 5.274961
[0/100][2519/2859] Loss: 5.272154
[0/100][2539/2859] Loss: 5.272564
[0/100][2559/2859] Loss: 5.270828
[0/100][2579/2859] Loss: 5.265246
[0/100][2599/2859] Loss: 5.263671
[0/100][2619/2859] Loss: 5.262466
[0/100][2639/2859] Loss: 5.257319
[0/100][2659/2859] Loss: 5.259255
[0/100][2679/2859] Loss: 5.254927
[0/100][2699/2859] Loss: 5.255911
[0/100][2719/2859] Loss: 5.255729
[0/100][2739/2859] Loss: 5.254725
[0/100][2759/2859] Loss: 5.251237
[0/100][2779/2859] Loss: 5.250115
[0/100][2799/2859] Loss: 5.247838
[0/100][2819/2859] Loss: 5.246548
[0/100][2839/2859] Loss: 5.247501
[0/100][2859/2859] Loss: 5.244867
[1/100][20/2859] Loss: 5.147863
[1/100][40/2859] Loss: 4.843151
[1/100][60/2859] Loss: 4.962826
[1/100][80/2859] Loss: 4.951033
[1/100][100/2859] Loss: 4.886427
[1/100][120/2859] Loss: 4.897883
[1/100][140/2859] Loss: 4.857891
[1/100][160/2859] Loss: 4.814690
[1/100][180/2859] Loss: 4.902147
[1/100][200/2859] Loss: 4.877947
[1/100][220/2859] Loss: 4.893171
[1/100][240/2859] Loss: 4.863156
[1/100][260/2859] Loss: 4.808864
[1/100][280/2859] Loss: 4.808995
[1/100][300/2859] Loss: 4.844473
[1/100][320/2859] Loss: 4.834679
[1/100][340/2859] Loss: 4.848883
[1/100][360/2859] Loss: 4.842553
[1/100][380/2859] Loss: 4.845419
[1/100][400/2859] Loss: 4.864552
